{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63575e70",
   "metadata": {},
   "source": [
    "# Cleaned & Fixed RAG Notebook\n",
    "\n",
    "This notebook is a cleaned, fixed, and fully rewritten RAG pipeline using a Groq chat model. It:\n",
    "\n",
    "- Shows required package installs\n",
    "- Loads PDFs from a folder and splits into chunks\n",
    "- Creates embeddings and a FAISS vectorstore\n",
    "- Builds a retriever\n",
    "- Uses a fixed `rag_simple` function compatible with chat LLMs (ChatGroq)\n",
    "\n",
    "> **Save** this file and run cells in order. If you run into missing package errors, run the first cell to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87970c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-groq langchain-core \"sentence-transformers>=2.2.2\" faiss-cpu PyPDF2 python-dotenv nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Groq + LangChain imports\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Document loading, splitting, embeddings, vectorstore\n",
    "try:\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.vectorstores import FAISS\n",
    "except Exception as e:\n",
    "    print(\"Warning: Could not import some langchain modules. If you see ImportError later, ensure langchain version is compatible.\")\n",
    "    print(e)\n",
    "\n",
    "from pathlib import Path\n",
    "print('Working directory:', os.getcwd())\n",
    "print('GROQ_API_KEY present in env:', bool(os.getenv('GROQ_API_KEY')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed59a13",
   "metadata": {},
   "source": [
    "## Initialize the Groq LLM\n",
    "Replace the model name if you have a different recommendation from Groq. The notebook uses `llama-3.1-8b-instant` as a recommended replacement for deprecated Gemma models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatGroq (change model_name if you prefer another supported model)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "if not groq_api_key:\n",
    "    print('Warning: GROQ_API_KEY not set. Set it in your .env file or your environment.')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "print('LLM initialized (ok)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdac679",
   "metadata": {},
   "source": [
    "## PDF processing: load all PDFs from a directory and split into chunks\n",
    "Place your PDFs into a folder named `pdfs/` (relative to this notebook) or change the `PDF_DIR` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd006562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your PDF directory here\n",
    "PDF_DIR = 'pdfs'  # change if your PDFs are elsewhere\n",
    "\n",
    "# Function to load and split PDFs\n",
    "from typing import List\n",
    "\n",
    "def process_all_pdfs(pdf_directory: str, chunk_size: int = 1000, chunk_overlap: int = 150):\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    if not pdf_dir.exists():\n",
    "        raise FileNotFoundError(f\"PDF directory not found: {pdf_directory}. Create the folder and add PDFs.\")\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob('**/*.pdf'))\n",
    "    print(f'Found {len(pdf_files)} PDF(s) in', pdf_directory)\n",
    "\n",
    "    loader = PyPDFLoader\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf in pdf_files:\n",
    "        print('Loading:', pdf)\n",
    "        try:\n",
    "            loader_instance = PyPDFLoader(str(pdf))\n",
    "            pages = loader_instance.load()\n",
    "            chunks = splitter.split_documents(pages)\n",
    "            print(f' - produced {len(chunks)} chunks')\n",
    "            all_docs.extend(chunks)\n",
    "        except Exception as e:\n",
    "            print('Failed to load or split', pdf, 'Error:', e)\n",
    "\n",
    "    print('Total chunks produced:', len(all_docs))\n",
    "    return all_docs\n",
    "\n",
    "# Ensure PDF_DIR exists\n",
    "Path(PDF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print('PDF_DIR exists:', Path(PDF_DIR).exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd9144",
   "metadata": {},
   "source": [
    "## Create embeddings and build FAISS vector store\n",
    "This cell will:\n",
    "- Create a HuggingFace embeddings object (uses sentence-transformers model)\n",
    "- Index the document chunks into FAISS\n",
    "\n",
    "**Note:** If you prefer another embedding model, change `EMBEDDING_MODEL_NAME`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embeddings and vectorstore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Choose an embeddings model (sentence-transformers). You can change model_name as needed.\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'  # 768-dim, good default\n",
    "\n",
    "print('Creating embedding model:', EMBEDDING_MODEL_NAME)\n",
    "emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Process PDFs and create vectorstore\n",
    "docs = process_all_pdfs(PDF_DIR)\n",
    "if len(docs) == 0:\n",
    "    print('No document chunks found. Add PDFs to the', PDF_DIR, 'directory and re-run this cell.')\n",
    "else:\n",
    "    vectorstore = FAISS.from_documents(docs, emb)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    try:\n",
    "        nvec = vectorstore.index.ntotal\n",
    "    except Exception:\n",
    "        nvec = 'unknown'\n",
    "    print('Vectorstore created. Number of vectors:', nvec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df148dd0",
   "metadata": {},
   "source": [
    "## Fixed RAG function\n",
    "This RAG function retrieves documents using the retriever, builds a safe prompt (no `.format()` on f-strings), and calls `llm.invoke()` with proper message objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_simple(query: str, retriever, llm, top_k: int = 3):\n",
    "    # retrieve docs (use retriever.get_relevant_documents for LangChain retrievers)\n",
    "    try:\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "    except Exception:\n",
    "        # fallback for other retriever interfaces\n",
    "        results = retriever.retrieve(query, top_k=top_k)\n",
    "        docs = []\n",
    "        # handle results that are dict-like\n",
    "        for r in results:\n",
    "            if hasattr(r, 'page_content'):\n",
    "                docs.append(r)\n",
    "            elif isinstance(r, dict) and 'content' in r:\n",
    "                from langchain.schema import Document\n",
    "                docs.append(Document(page_content=r['content']))\n",
    "\n",
    "    if not docs:\n",
    "        print('Retrieved 0 documents (after filtering)')\n",
    "        return 'No relevant context found to answer the question.'\n",
    "\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use the following context to answer the question concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Call chat model with message objects\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content='You are a helpful assistant. Answer concisely using only the provided context.'),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "\n",
    "    # response may be an AIMessage or similar; extract content safely\n",
    "    try:\n",
    "        return response.content\n",
    "    except Exception:\n",
    "        # fallback to str(response)\n",
    "        return str(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dec42",
   "metadata": {},
   "source": [
    "## Example query\n",
    "Run the following cell to test your RAG pipeline once the vectorstore and retriever are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (run after creating vectorstore & retriever)\n",
    "try:\n",
    "    answer = rag_simple('What is attention all you need?', retriever, llm)\n",
    "    print('\\nAnswer:\\n', answer)\n",
    "except NameError as e:\n",
    "    print('Make sure you ran the cell that builds the vectorstore and retriever. Error:', e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
